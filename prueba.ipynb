{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "from wsgiref import validate\n",
    "import pandas as pd \n",
    "from config import path\n",
    "from dics import gcons, f12_vars, f3_vars, f11_vars, nc_vars, siebel_vars, ro_vars, mc_vars, en_vars, q_vars, siebel_vars, tesor_nc, tesor_sieb\n",
    "from os import listdir\n",
    "from unidecode import unidecode\n",
    "from datetime import datetime\n",
    "\n",
    "class CONC:\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    def  __init__(self) -> None:\n",
    "        dcorte = '220816' #TODO input('Ingrese corte: ')\n",
    "        self.path_input = path + '/input' + '/'+ dcorte \n",
    "        self.path_output = path + '/output'\n",
    "        files = listdir(self.path_input)\n",
    "        self.file_name_list = ['f3', 'f11', 'f12', 'nc', 'ro', 'mc', 'en', 'quiebres', 'ss', 'tesoreria_ntc', 'tesoreria_sieb']\n",
    "        self.file_name_dirs = get_dirs(files, self.file_name_list)\n",
    "\n",
    "    def run(self):\n",
    "        # Run \n",
    "        print('Cargando archivos ...')\n",
    "        self.load_files()\n",
    "        print('Transformando archivos ...')\n",
    "        self.transform_files()\n",
    "        print('Realizando uniones ...')\n",
    "        self.res = self.get_join()\n",
    "        print('Clasificando archivos ...')\n",
    "        df = self.f12_classifier()\n",
    "        print('Guardando archivos ...')\n",
    "        df.to_csv(f'{self.path_output}/220817_resultado.csv', sep=';', index=False)\n",
    "        df.to_csv(f'{self.path_output}/220817_resultado_v2.csv', index=False)\n",
    "        print('Finalizado')\n",
    "\n",
    "\n",
    "    def load_files(self):\n",
    "        self.dfs = []\n",
    "        for dir in self.file_name_dirs: \n",
    "            if dir != '':\n",
    "                self.dfs.append(pd.read_csv(f'{self.path_input}/{dir}', sep=';', dtype='object'))\n",
    "            else:\n",
    "                self.dfs.append(pd.DataFrame())\n",
    "\n",
    "    def transform_files(self):\n",
    "        # PAP:  f3, f12, ro, en, siebel\n",
    "        # MAP: f11, nc, mc, quiebres \n",
    "        col_key = [f3_vars['key'], f11_vars['key'], f12_vars['key'], nc_vars['key'], ro_vars['key'], mc_vars['key'], en_vars['key'], q_vars['key'], siebel_vars['key'], tesor_nc['key'], tesor_sieb['key']]\n",
    "        cols_dup = [f3_vars['dkeys'], f11_vars['dkeys'], f12_vars['dkeys'], nc_vars['dkeys'], ro_vars['dkeys'], mc_vars['dkeys'], en_vars['dkeys'], q_vars['dkeys'], siebel_vars['dkeys'], tesor_nc['dkeys'], tesor_sieb['dkeys']]\n",
    "        cols_num = [f3_vars['cnum'], f11_vars['cnum'], f12_vars['cnum'], nc_vars['cnum'], ro_vars['cnum'], mc_vars['cnum'], en_vars['cnum'], q_vars['cnum'], siebel_vars['cnum'], tesor_nc['cnum'], tesor_sieb['cnum']]\n",
    "        cols_string = [f3_vars['cstring'], f11_vars['cstring'], f12_vars['cstring'], nc_vars['cstring'], ro_vars['cstring'], mc_vars['cstring'], en_vars['cstring'], q_vars['cstring'], siebel_vars['cstring'], tesor_nc['cstring'], tesor_sieb['cstring']]\n",
    "        cols_to_keep = [f3_vars['ckeep'], f11_vars['ckeep'], f12_vars['ckeep'], nc_vars['ckeep'], ro_vars['ckeep'], mc_vars['ckeep'], en_vars['ckeep'], q_vars['ckeep'], siebel_vars['ckeep'], tesor_nc['ckeep'], tesor_sieb['ckeep']]\n",
    "\n",
    "        for i, df in enumerate(self.dfs):\n",
    "            if df.empty == False: \n",
    "                self.dfs[i] = set_prefijo_df(self.dfs[i], self.file_name_list[i])\n",
    "                self.dfs[i] = keep_cols(self.dfs[i], cols_to_keep[i])\n",
    "                self.dfs[i] = clean_num(self.dfs[i], cols_num[i])\n",
    "                self.dfs[i] = clean_string(self.dfs[i], cols_string[i])\n",
    "                self.dfs[i] = delete_null_values(self.dfs[i], col_key[i])\n",
    "                self.dfs[i] = delete_duplicates(self.dfs[i], cols_dup[i])             \n",
    "        \n",
    "        # Transformaciones adicionales \n",
    "        self.dfs[2].loc[:, f12_vars['fpactada']] = pd.to_datetime(self.dfs[2][f12_vars['fpactada']], format='%Y-%m-%d')\n",
    "        cond_fpactada = (self.dfs[2][f12_vars['fpactada']]< datetime.now() )& (self.dfs[2][f12_vars['fpactada']].notna())\n",
    "        self.dfs[2].loc[cond_fpactada, f12_vars['ind_fpactada']] = 'OVERDUE'\n",
    "        self.dfs[2].loc[~cond_fpactada, f12_vars['ind_fpactada']] = 'ON TIME'\n",
    "\n",
    "    def get_dfs(self):\n",
    "        return self.dfs \n",
    "\n",
    "    def f12_classifier(self):\n",
    "        df = self.res.copy()\n",
    "\n",
    "        val_entregas = df[f3_vars['key']].notna() | df[f11_vars['key']].notna() | df[mc_vars['key']].notna() | df[en_vars['key']].notna() | df[q_vars['key']].notna()\n",
    "        df.loc[val_entregas, 'gco_ind_entregas'] = 'Tiene registro RO | MC | F3 | F11 | QUIEBRE'\n",
    "\n",
    "        df.loc[df[siebel_vars['key']].notna() , 'gco_ind_ss'] = 'Tiene SS'\n",
    "        df.loc[df['ss_n3'].str.contains(r'TROCADO|AVERIA|INCOMPLETO|ENTREGA FALSA', na = False), 'gco_ind_ss_n3'] = 'Se encontró TROCADO|AVERIA|INCOMPLETO|ENTREGA FALSA en SS N3'\n",
    "\n",
    "        df.loc[(df['tesoreria_ntc_cod aut nc'].notna()) | ((df['tesoreria_sieb_ss'].notna())), 'gco_ind_registra_pago'] = 'y'\n",
    "        df.loc[df['gco_ind_registra_pago'].isna() ] = 'n'\n",
    "\n",
    "        total_entrega = get_id_values(df, f12_vars['key'], f12_vars['estado'], ['TOTAL ENTREGA'])\n",
    "\n",
    "        # C1 ENTREGA ADMINISTRATIVA\n",
    "        te_ent_admin = get_id_values(df, f12_vars['key'], f12_vars['subestado'], ['ENTREGA ADMINISTRATIVO'], total_entrega)\n",
    "        mts_val = ['EN PROCESO NC', 'ENTREGADO EN TIENDA', 'FALTA STOCK', 'RETORNADO A ORIGEN', 'REFACTURACION', 'PROD CON NCRD']\n",
    "        c1 = get_id_values(df, f12_vars['key'], f12_vars['mt'], mts_val, te_ent_admin)\n",
    "\n",
    "        # C2 ENTREGAS         \n",
    "        entregas = ['ENTREGA POR PDA', 'ENTREGA EN SRX', 'ENTREGA PROVEEDOR']\n",
    "        te_entregas = get_id_values(df, f12_vars['key'], f12_vars['subestado'], entregas, total_entrega)\n",
    "        c2 = get_id_values(df, f12_vars['key'], 'gco_ind_entregas', ['Tiene registro RO | MC | F3 | F11 | QUIEBRE'], te_entregas)\n",
    "    \n",
    "        # C3 NO ENTREGAS \n",
    "        no_entregas = [ 'ENTREGA EN TIENDA','EN LINEA PRV. CON FACTURA', 'EN LINEA PRV. PEND. FACTURA', 'TOTAL ENTREGA']\n",
    "        te_no_entregas = get_id_values(df, f12_vars['key'], f12_vars['subestado'], no_entregas, total_entrega)\n",
    "        c3 =  get_id_values(df, f12_vars['key'], 'gco_ind_ss_n3', ['Se encontró TROCADO|AVERIA|INCOMPLETO|ENTREGA FALSA en SS N3'], te_no_entregas)\n",
    "\n",
    "        # C4 ANULADO POR NCRD\n",
    "        ancdr_est = get_id_values(df, f12_vars['key'], f12_vars['estado'], ['ANULADO X NCRD'] )\n",
    "        c4 = get_id_values(df, f12_vars['key'], f12_vars['subestado'], ['ANULADO X NCRD'], ancdr_est)\n",
    "\n",
    "        # C5 EN RUTA Y DIGITADO \n",
    "        ## EN RUTA\n",
    "        en_ruta = get_id_values(df, f12_vars['key'], f12_vars['estado'], ['EN RUTA'] )\n",
    "        er_se_values =['EN RUTA AL CLIENTE', 'MT PROVEEDOR', 'EN TRANSITO', 'MT']\n",
    "        er_se = get_id_values(df, f12_vars['key'], f12_vars['subestado'], er_se_values , en_ruta)\n",
    "        c5_p1 = get_id_values(df, f12_vars['key'], f12_vars['ind_fpactada'], ['OVERDUE'], er_se)\n",
    "\n",
    "        ## DIGITADO\n",
    "        digitados = get_id_values(df, f12_vars['key'], f12_vars['estado'], ['DIGITADO'] )\n",
    "        dig_se_values =['BOLETEADO/FACTURADO', 'MT PROVEEDOR']\n",
    "        dig_se = get_id_values(df, f12_vars['key'], f12_vars['subestado'], dig_se_values, digitados)\n",
    "        c5_p2 =get_id_values(df, f12_vars['key'], f12_vars['ind_fpactada'], ['OVERDUE'], dig_se)\n",
    "\n",
    "        # RESULTADO \n",
    "        f12s_validos = c1 + c2 + c3 + c4 + c5_p1 + c5_p2\n",
    "        return set_colvalue(df, f12_vars['key'], f12s_validos, 'gco_comment', 'Aplica conciliación')\n",
    "    \n",
    "    def get_join(self):\n",
    "        \n",
    "        # Uniones previas \n",
    "        self.dfs[1]['f11_folio_f12'] = self.dfs[1]['f11_observacion'].str.extract(r'^([1][2]\\d{7,})') \n",
    "        self.dfs[1] = self.dfs[1].loc[~self.dfs[1]['f11_folio_f12'].isna()].reset_index(drop=True)\n",
    "        self.dfs[1].drop_duplicates(['f11_folio_f12', 'f11_upc'], inplace=True)\n",
    "        ne_ro = join( self.dfs[4], self.dfs[6],  'ro_ro', 'en_centrada', 'many_to_one') \n",
    "\n",
    "        # Inicio \n",
    "        f12_nc = join(self.dfs[2], self.dfs[3], ['f12_nfolio', 'f12_prd_upc'], ['nc_nfolio','nc_prod_ean_id'], 'one_to_one')\n",
    "\n",
    "        # Uniendo base de SS por sub orden y nro f12\n",
    "        f12_ss_1 = join(f12_nc, self.dfs[8], 'f12_so', 'ss_suborden', 'many_to_one')\n",
    "        ss_x_f12 = self.dfs[8].drop_duplicates(['ss_num_f12']) \n",
    "        f12_ss = f12_ss_1.merge(ss_x_f12, how = 'left', left_on = 'f12_nfolio',right_on = 'ss_num_f12', suffixes=('', '_y')) \n",
    "        f12_ss.loc[(f12_ss['ss_ss_y'].notna()), siebel_vars['ckeep'] ] = f12_ss.loc[(f12_ss['ss_ss_y'].notna()), gcons['union_ss_aux']].values\n",
    "        f12_ss.drop(columns = gcons['union_ss_aux'], inplace = True) # Se eliminan columnas de segunda Base\n",
    "\n",
    "        # Uniendo base tesoreria_nc por cautoriza y ss \n",
    "        f12_tes_ntc_1 = join(f12_ss, self.dfs[9], 'nc_cautoriza_nc', 'tesoreria_ntc_cod aut nc', 'many_to_one')\n",
    "        tes_ntc_x_ss = self.dfs[9].drop_duplicates(['tesoreria_ntc_ss']) \n",
    "        f12_tes_ntc = f12_tes_ntc_1.merge(tes_ntc_x_ss, how = 'left', left_on = 'ss_num_f12', right_on = 'tesoreria_ntc_ss', suffixes=('', '_y')) \n",
    "        f12_tes_ntc.loc[(f12_tes_ntc['tesoreria_ntc_ss_y'].notna()), tesor_nc['ckeep']] = f12_tes_ntc.loc[(f12_ss['tesoreria_ntc_ss_y'].notna()), gcons['union_tes_ntc_aux']].values\n",
    "        f12_tes_ntc.drop(columns = gcons['union_tes_ntc_aux'], inplace = True) # Se eliminan columnas de segunda Base\n",
    "\n",
    "        # #Continuan unioines\n",
    "        f12_tes_ss = join(f12_tes_ntc, self.dfs[10], 'ss_ss', 'tesoreria_sieb_ss', 'many_to_one')\n",
    "        f12_ro = join(f12_tes_ss, ne_ro, ['f12_nfolio', 'f12_prd_upc'] , ['ro_do_inicial', 'ro_upc'], 'one_to_one')\n",
    "        f12_mc = join(f12_ro, self.dfs[5], 'f12_nfolio' , 'mc_entrada', 'many_to_one')\n",
    "        f12_f11 = join(f12_mc, self.dfs[1], ['f12_nfolio', 'f12_prd_upc'] , ['f11_folio_f12', 'f11_upc'], 'one_to_one')\n",
    "        f12_f3 = join(f12_f11, self.dfs[0], ['f12_nfolio', 'f12_prd_upc'], ['f3_folio_f12', 'f3_upc'], 'one_to_one')\n",
    "        f12_q = join(f12_f3, self.dfs[7], ['f12_nfolio', 'f12_prd_upc'], ['quiebres_f12', 'quiebres_codigo_barras'], 'one_to_one')\n",
    "        return f12_q\n",
    "\n",
    "def get_id_values(df, id_col, col, values, init_ids = []):\n",
    "    todos = df[id_col].unique() if len(init_ids) == 0 else init_ids\n",
    "    return list(df.loc[df[id_col].isin(todos) &  df[col].isin(values), id_col].unique())\n",
    "\n",
    "def set_colvalue(df, id_col, ids,  comment_col, comment_value): \n",
    "    res = df.copy()\n",
    "    res.loc[res[id_col].isin(ids), comment_col] = comment_value\n",
    "    return res\n",
    "\n",
    "def set_prefijo_df(df, prefijo): # TODO reescribir en list coprehension\n",
    "    col = {}\n",
    "    for i in df.columns:\n",
    "        col[i] = f'{prefijo}_{i.lower()}'\n",
    "    return df.rename(columns=col)\n",
    "\n",
    "def delete_null_values(df, col): # PAP [x]\n",
    "    return df.loc[~df[col].isna()].reset_index(drop=True)\n",
    "\n",
    "def delete_duplicates(df, col_list): # MAP\n",
    "    return df.drop_duplicates(col_list)\n",
    "\n",
    "def clean_num(df, col_list): # PAP\n",
    "    res = df.copy()\n",
    "    for col in col_list:\n",
    "        res.loc[:, col] = res[col].str.replace(r'([^0-9,])', '', regex=True)\n",
    "        res.loc[:, col] = pd.to_numeric(res.loc[:, col] )\n",
    "    return res\n",
    "\n",
    "def clean_string(df, col_list): # MAP\n",
    "    res = df.copy()\n",
    "    for col in col_list:\n",
    "        res[col] = res[col].fillna('nan')\n",
    "        res[col] = res[col].apply(unidecode)\n",
    "        res[col] = res[col].str.replace(r'([^a-zA-Z0-9-+(). ])', '', regex=True)\n",
    "        res[col] = res[col].str.strip()\n",
    "        res[col] = res[col].str.upper()\n",
    "    return res\n",
    "\n",
    "def keep_cols(df, cols_to_keep):\n",
    "    return df.loc[:, cols_to_keep].reset_index(drop=True)\n",
    "\n",
    "def get_dirs(files, file_name_list): # PAP [x]\n",
    "    lista = []\n",
    "    for name in file_name_list:\n",
    "        flag = False \n",
    "        for file in files:\n",
    "            if file.__contains__(name): \n",
    "                lista.append(file)\n",
    "                flag = True\n",
    "        if flag: \n",
    "            continue \n",
    "        else: \n",
    "            lista.append('')\n",
    "    return lista \n",
    "\n",
    "def join(df_left, df_right, col_df_left, col_df_right, val_type):\n",
    "    return df_left.merge(df_right, how = 'left', left_on = col_df_left, right_on = col_df_right, validate=val_type)\n",
    "\n",
    "def innit_commandline():\n",
    "    conc = CONC()\n",
    "    conc.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conc = CONC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.transform_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Anulación de compra total', 'OMS - Derecho Retracto-Retiro',\n",
       "       'Estado del Despacho/Retiro', 'Estado del reclamo',\n",
       "       'Incumplimiento fecha Entrega', 'Consulta NC',\n",
       "       'Estado de Orden de Compra', 'OMS - Devolución por NC - PNC',\n",
       "       'Copia o Cambio de Boleta/FC', 'Incumplimiento Sin Stock',\n",
       "       'Consulta de Fecha y Horario', 'Atención Click and Collect',\n",
       "       'Derivación a Tienda - DEV', 'Garantia no cubre falla STE',\n",
       "       'Dev x NC en tienda', 'Incumplimiento Entrega Falso',\n",
       "       'Devolución Parcial de compra', 'Derivación a Tienda - PNC',\n",
       "       'Procedimiento y Politicas', 'Devolucion no acreditada en TC',\n",
       "       'OMS - Devolución por NC - PI', 'Anulacion parcial de compra',\n",
       "       'Incumplimiento F12 atrasado', 'OMS - E/R mismo Producto - PI',\n",
       "       'OMS - E/R mismo Producto - PD', 'Publicidad engañosa',\n",
       "       'OMS - E/R mismo Producto - PNC', 'Problema con Página web',\n",
       "       'Consulta Serv Tecnico Externo', 'OMS - Devolución por NC - PD',\n",
       "       'OMS - Envío de mismo SKU', 'Otras Areas', 'Solicitud de armado',\n",
       "       'Garantia no cubre falla ST', 'FC / NC Administrativa',\n",
       "       'Problema con el Transportista', 'OMS - E/R mismo Producto - PF',\n",
       "       'Cambio Dirección de entrega', 'Dev de despachos',\n",
       "       'OMS - Devolución por NC - DEV', 'Desmonte de Puerta',\n",
       "       'E/R mismo Producto - PF', 'OMS - Retiro para Serv Técnico',\n",
       "       'Cambio Producto en TDA - PD', 'Reclamo Instalación Electro',\n",
       "       'Derivación a Tienda - PD', 'OMS - Anulación compra parcial',\n",
       "       'Ventas', 'Garantías', 'Dotaciones tienda',\n",
       "       'Atraso de emisión de factura', 'Aviso seguridad producto',\n",
       "       'Cambio Producto en TDA - DEV', 'Procedimiento Retiro en Tienda',\n",
       "       'Anuladas no cumple condición', 'Consulta de Stock',\n",
       "       'OMS - Devolución por NC - PF', 'Solicitud Instalación Gimnasio',\n",
       "       'Consulta de armado', 'Cambio Producto en TDA - PNC',\n",
       "       'Consulta de Estado STE', 'Horario y Teléfono de Tiendas',\n",
       "       'Error en cobro tarjeta CMR', 'Error en cobro tarjeta externa',\n",
       "       'Derivación a Tienda - PF', 'Solicitud Instalación Electro',\n",
       "       'Dev x garantía', 'Reclamo Click and Collect',\n",
       "       'Problema con Usuario web/Clave', 'OMS - E/R distinto Producto PD',\n",
       "       'Cambio Fecha de entrega', 'Consulta Serv Tecnico Electro',\n",
       "       'Reactivación Gift Card', 'Eventos y promociones',\n",
       "       'Incumplimiento fecha Retiro', 'Consulta de Inst Electro',\n",
       "       'Generales por Web', 'Consulta de Descripción',\n",
       "       'Consulta de Estado ST', 'Llamada muda o caida', 'CMR',\n",
       "       'Derivación a Tienda - PI', 'Consulta de Inst Gimnasio',\n",
       "       'Despacho-Bodega', 'Mala atencion ST', 'Dev intereses',\n",
       "       'Cambio fecha de STE', 'Consulta de Funcionamiento',\n",
       "       'Incumplimiento fecha Cambio', 'OMS - Cambio Dirección entrega',\n",
       "       'Consulta sobre Garantía Ext', 'Banco Falabella',\n",
       "       'Incpmto Políticas Devoluciones', 'Consulta rechazo OC',\n",
       "       'Trx rechazada pagada o abonada', 'Cambio fecha de ST',\n",
       "       'Consulta costo Flete', 'Atención Comercial - PD',\n",
       "       'Seguros Falabella', 'Entrega complemento en Tienda',\n",
       "       'Incumplimiento de STE', 'Consulta de Gift Card',\n",
       "       'Reclamo por Servicio de armado', 'Atención Comercial - PI',\n",
       "       'Procedimientos Boletas y Compr', 'Incumplimiento de ST',\n",
       "       'SAC Tienda', 'Consulta Novios', 'OMS - E/R distinto Producto PI',\n",
       "       'Cupo Retenido', 'Atención Comercial - PNC', 'Mala atencion STE',\n",
       "       'Devolución por NC - PF', 'Reclamo Instalación Gimnasio',\n",
       "       'Click & Collect'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conc.dfs[8]['ss_n3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.dfs[8].loc[conc.dfs[8]['ss_n3'].str.contains(r'TROCADO|AVERIA|INCOMPLETO|ENTREGA FALSA', na = False) , 'gco_ind_ss_n3'] = 'Se encontro TROCADO | AVERIA | INCOMPLETO | ENTREGA FALSA en SS nivel 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conc.dfs[1]['f11_folio_f12'] = conc.dfs[1]['f11_observacion'].str.extract(r'^([1][2]\\d{7,})') \n",
    "# conc.dfs[1] = conc.dfs[1].loc[~conc.dfs[1]['f11_folio_f12'].isna()].reset_index(drop=True)\n",
    "# conc.dfs[1].drop_duplicates(['f11_folio_f12', 'f11_upc'], inplace=True)\n",
    "# ne_ro = join( conc.dfs[4], conc.dfs[6],  'ro_ro', 'en_centrada', 'many_to_one') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f12_nc = join(conc.dfs[2], conc.dfs[3], ['f12_nfolio', 'f12_prd_upc'], ['nc_nfolio','nc_prod_ean_id'], 'one_to_one')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3818599390.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [10]\u001b[1;36m\u001b[0m\n\u001b[1;33m    df.['area_responsable'] = ''\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.['area_responsable'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc.dfs[8].loc[conc.dfs[8]['ss_n3'].str.contains(r'TROCADO|AVERIA|INCOMPLETO|ENTREGA FALSA', na = False), 'gco_ind_ss_n3'] = 'Se encontro TROCADO | AVERIA | INCOMPLETO | ENTREGA FALSA en SS nivel 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 130, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m220817_resultado.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3_2\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 130, saw 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('220817_resultado.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c043d662fbed7cdb1a55c38793736f50ece9eb5b859cfa43e78ca91054ae88a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
